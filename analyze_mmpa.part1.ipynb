{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def applyParallel(df, func):\n",
    "    df_split = np.array_split(df, cpu_count())\n",
    "    pool = Pool(cpu_count())\n",
    "    data = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def normalize_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=False)\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "def normalize_smiles_series(smiles_series):\n",
    "    return smiles_series.map(normalize_smiles)\n",
    "\n",
    "\n",
    "def generate_murcko_scaffold(smile):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol:\n",
    "            scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "            return Chem.MolToSmiles(scaffold, isomericSmiles=False)\n",
    "        else:\n",
    "            return np.NaN\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "\n",
    "def generate_topological_scaffold(smile):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol:\n",
    "            scaffold = MurckoScaffold.MakeScaffoldGeneric(MurckoScaffold.GetScaffoldForMol(mol))\n",
    "            return Chem.MolToSmiles(scaffold, isomericSmiles=False)\n",
    "        else:\n",
    "            return np.NaN\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "    \n",
    "                   \n",
    "def generate_murcko_scaffold_series(data):\n",
    "    return data.map(generate_murcko_scaffold)\n",
    "\n",
    "def generate_topological_scaffold_series(data):\n",
    "    return data.map(generate_topological_scaffold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True,fastmath=True)\n",
    "def find_nns(fps, references):\n",
    "    all_nns = np.zeros((fps.shape[0], 3), dtype=np.float64)\n",
    "    \n",
    "    for i in prange(fps.shape[0]):\n",
    "        nn_dist, nn_idx, avg_nn = _find_nn_and_avg_nn(fps[i], references)\n",
    "        all_nns[i,0] = nn_dist\n",
    "        all_nns[i,1] = nn_idx\n",
    "        all_nns[i,2] = avg_nn\n",
    "    \n",
    "    return all_nns\n",
    "    \n",
    "@njit(fastmath=True)\n",
    "def _find_nn_and_avg_nn(fp, fps):\n",
    "    nn_dist = 0.0\n",
    "    nn_idx = numba.int32(-1)\n",
    "    avg_nn = 0.0\n",
    "    \n",
    "    for i in range(fps.shape[0]):\n",
    "        tanimoto = _minmax_two_fp(fp, fps[i])\n",
    "        avg_nn += tanimoto\n",
    "        if tanimoto > nn_dist:\n",
    "            nn_dist = tanimoto\n",
    "            nn_idx = i\n",
    "    avg_nn = numba.float64(avg_nn) / numba.float64(fps.shape[0])\n",
    "            \n",
    "    return nn_dist, nn_idx, avg_nn\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _minmax_two_fp(fp1, fp2):\n",
    "    common = numba.int32(0)\n",
    "    maxnum = numba.int32(0)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(fp1):\n",
    "        min_ = fp1[i]\n",
    "        max_ = fp2[i]\n",
    "\n",
    "        if min_ > max_:\n",
    "            min_ = fp2[i]\n",
    "            max_ = fp1[i]\n",
    "\n",
    "        common += min_\n",
    "        maxnum += max_\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return numba.float64(common) / numba.float64(maxnum)\n",
    "\n",
    "def counted_tanimoto_similarity(f1, fp2, return_distance=True):\n",
    "    if return_distance:\n",
    "        return 1. - _minmax_two_fp(fp1,fp2)\n",
    "    else:\n",
    "        return _minmax_two_fp(fp1,fp2)\n",
    "    \n",
    "def bulk_counted_tanimoto(fp1, fps, return_distance=True):\n",
    "    if return_distance:\n",
    "        return [1. - _minmax_two_fp(fp1,fp2) for fp2 in fps]\n",
    "    else: \n",
    "        return [_minmax_two_fp(fp1,fp2) for fp2 in fps]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_FILES = False\n",
    "\n",
    "\n",
    "def read_scaffold_memory(folder):\n",
    "    memory = pd.read_csv(f\"data/memories/{folder}/memory_preprocessed.csv.gz\")\n",
    "    return memory\n",
    "\n",
    "def read_id_steps(folder):\n",
    "    return read_scaffold_memory(folder)[[\"ID\",\"step\",\"total_score\"]]\n",
    "\n",
    "\n",
    "def read_mmp_and_filter_pairs(mmpfile):\n",
    "    foldername = mmpfile.split(\"/\")[1]\n",
    "    target = foldername.split(\"_\")\n",
    "\n",
    "    filtername = target[1]\n",
    "    target = target[0]\n",
    "\n",
    "    df = pd.read_csv(mmpfile, header=None)\n",
    "    df.columns = [\"SMILES_OF_LEFT_MMP\",\"SMILES_OF_RIGHT_MMP\",\"ID_OF_LEFT_MMP\",\"ID_OF_RIGHT_MMP\",\"SMIRKS_OF_TRANSFORMATION\",\"SMILES_OF_CONTEXT\"]\n",
    "\n",
    "    df[\"SET_OF_LEFT_MMP\"] = df[\"ID_OF_LEFT_MMP\"].map(lambda x: x.split(\"_\")[-2] )\n",
    "    df[\"SET_OF_RIGHT_MMP\"] = df[\"ID_OF_RIGHT_MMP\"].map(lambda x: x.split(\"_\")[-2] )\n",
    "\n",
    "\n",
    "    mmp_pairs = df.query(\" SET_OF_LEFT_MMP == @filtername and (SET_OF_RIGHT_MMP == 'training' or SET_OF_RIGHT_MMP == 'test' or SET_OF_RIGHT_MMP == 'validation' )\")\n",
    "    ids = read_id_steps(foldername)\n",
    "    \n",
    "    mmp_pairs = pd.merge(mmp_pairs, ids, left_on=\"ID_OF_LEFT_MMP\", right_on=\"ID\", how=\"inner\")\n",
    "    del mmp_pairs[\"ID\"]\n",
    "    mmp_pairs.rename(columns={\"step\": \"STEP\", \"total_score\": \"SCORE\"}, inplace=True)\n",
    "    mmp_pairs = mmp_pairs.sort_values(by=['STEP'])\n",
    "    return mmp_pairs\n",
    "\n",
    "\n",
    "for mmpfile in glob(\"MMP/*/MMP.csv.gz\"):\n",
    "    folder = mmpfile.split(\"/\")[1]\n",
    "    if len(folder.split(\"_\")) > 2: #we only process the \"default\" parameters here in this notebook\n",
    "        continue \n",
    "    mmpfile_filtered = mmpfile.replace(\"MMP.csv.gz\", \"MMP_filtered.csv\")\n",
    "    if not os.path.exists(mmpfile_filtered) or OVERWRITE_FILES:\n",
    "        mmp = read_mmp_and_filter_pairs(mmpfile)\n",
    "        mmp.to_csv(mmpfile_filtered, index=False)\n",
    "    else:\n",
    "        print(f\"Skipping {mmpfile} as it seems to already be processed\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mmp_stats(mmp_pairs):\n",
    "        nb_mmp_pairs = len(mmp_pairs)\n",
    "        nb_generated_unique_cpds = len(mmp_pairs.drop_duplicates(\"SMILES_OF_LEFT_MMP\"))\n",
    "        return nb_mmp_pairs, nb_generated_unique_cpds\n",
    "    \n",
    "\n",
    "def get_mmp_cpd(df, filtername, maxstep, minactivity):\n",
    "    mmp_pairs = df.query(\"SET_OF_LEFT_MMP == @filtername and STEP < @maxstep and SCORE >= @minactivity\").copy()\n",
    "    \n",
    "    mmp_pairs = mmp_pairs.sort_values(by=['STEP'])\n",
    "    mmp_pairs = mmp_pairs.drop_duplicates([\"SMILES_OF_LEFT_MMP\", \"SMILES_OF_RIGHT_MMP\"], keep='first')\n",
    "        \n",
    "    np_mmp_pairs_training, nb_generated_unique_cpds_training = _get_mmp_stats(mmp_pairs.query(\"SET_OF_RIGHT_MMP == 'training'\"))\n",
    "    np_mmp_pairs_test, nb_generated_unique_cpds_test = _get_mmp_stats(mmp_pairs.query(\"SET_OF_RIGHT_MMP == 'test' or SET_OF_RIGHT_MMP == 'validation'\"))\n",
    "    \n",
    "    return np_mmp_pairs_training, nb_generated_unique_cpds_training, np_mmp_pairs_test, nb_generated_unique_cpds_test#, np_mmp_pairs_validation, nb_generated_unique_cpds_validation\n",
    "\n",
    "\n",
    "def load_mmp(target, filtername, maxstep, minactivity):\n",
    "    file = glob(f\"MMP/{target}_{filtername}/MMP_filtered.csv\")[0]\n",
    "    df = pd.read_csv(file)\n",
    "    return get_mmp_cpd(df, filtername, maxstep, minactivity)\n",
    "\n",
    "\n",
    "target_params = {\n",
    "    \"DRD2\":      {\"maxstep\": 300,\n",
    "                  \"minactivity\": 0.7},\n",
    "    \"HTR1A\":     {\"maxstep\": 300,\n",
    "                  \"minactivity\": 0.7},\n",
    "    \"clogP\":     {\"maxstep\": 150,\n",
    "                  \"minactivity\": 1.}\n",
    "}\n",
    "\n",
    "clogP_param = {\n",
    "        \"clogP\": {\"maxstep\": 150,\n",
    "                  \"minactivity\": 1.}, \n",
    "}\n",
    "\n",
    "filternames = [\"NoFilter\", \"CompoundSimilarity\", \"IdenticalMurckoScaffold\", \"IdenticalTopologicalScaffold\", \"ScaffoldSimilarity\"]\n",
    "filternames_in_plots = [ \"No memory\", \"CompoundSimilarity memory\", \"IdenticalMurckoScaffold memory\", \"IdenticalCarbonSkeleton memory\", \"ScaffoldSimilarity memory\"]\n",
    "\n",
    "mmp_memories = {}\n",
    "for target, params in target_params.items():\n",
    "    if target not in mmp_memories:\n",
    "        mmp_memories[target] = {} \n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']\n",
    "    for filtername in filternames:\n",
    "        nb_mmp_pairs_training, nb_generated_unique_cpds_training, np_mmp_pairs_test, nb_generated_unique_cpds_test = load_mmp(target, filtername, maxstep, minactivity)\n",
    "        print(f\"{target} {filtername} STEPS: {maxstep}\")\n",
    "        print(f\"Training: {nb_mmp_pairs_training} MMPs formed by generating {nb_generated_unique_cpds_training} compounds.\")\n",
    "        print(f\"Test: {np_mmp_pairs_test} MMPs formed by generating {nb_generated_unique_cpds_test} compounds.\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        mmp_memories[target][filtername] = {} \n",
    "        folder = glob(f\"MMP/{target}_{filtername}\")[0].split(\"/\")[-1]\n",
    "        memory = read_scaffold_memory(folder)\n",
    "        memory.rename(columns={\"step\": \"STEP\", \"total_score\": \"SCORE\"}, inplace=True)\n",
    "        memory = memory.sort_values(by=['STEP'])\n",
    "        mmp_memories[target][filtername] = memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in clogP_param.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']\n",
    "    print(f\"Number of MMP analogs\\t\\t\\tgen\\tcore\\tpercent_core\")\n",
    "    for filtername in filternames:\n",
    "        file = glob(f\"MMP/{target}_{filtername}/MMP_filtered.csv\")[0]\n",
    "        df = pd.read_csv(file)\n",
    "        mmp_pairs = df.query(\"SET_OF_LEFT_MMP == @filtername and STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        unique_mmp_pairs = mmp_pairs.drop_duplicates([\"ID_OF_LEFT_MMP\",\"ID_OF_RIGHT_MMP\"])\n",
    "        generated_cpds = unique_mmp_pairs.drop_duplicates(\"ID_OF_LEFT_MMP\")\n",
    "        covered_cores = unique_mmp_pairs.drop_duplicates(\"ID_OF_RIGHT_MMP\")\n",
    "        nb_generated_cpds = len(generated_cpds)\n",
    "        nb_covered_cores = len(covered_cores)\n",
    "        percent = nb_covered_cores/ 487 * 100\n",
    "\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb_generated_cpds}\\t{nb_covered_cores}\\t{percent:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_FILES = False\n",
    "\n",
    "\n",
    "def cecfp6_from_mol(mol):\n",
    "    fp = AllChem.GetMorganFingerprint(mol, 3, useCounts=True, useFeatures=False)\n",
    "    size = 2048\n",
    "    nfp = np.zeros(size, np.int32)\n",
    "    for idx,v in fp.GetNonzeroElements().items():\n",
    "        nidx = idx%size\n",
    "        nfp[nidx] += int(v)\n",
    "    return nfp\n",
    "\n",
    "def cecfp6_from_smiles(smiles):\n",
    "    return cecfp6_from_mol(Chem.MolFromSmiles(smiles))\n",
    "\n",
    "def generate_fingerprint_series(data):\n",
    "    return data.map(cecfp6_from_smiles)\n",
    "\n",
    "\n",
    "def find_nns_from_pandas(fps, reference):\n",
    "    fps = np.array([np.array(e) for e in fps.values],dtype=np.int32)\n",
    "    reference = np.array([np.array(e) for e in reference.values],dtype=np.int32)\n",
    "    return find_nns(fps, reference)\n",
    "\n",
    "\n",
    "for target, params in target_params.items():\n",
    "    reference = pd.read_pickle(f\"data/{target}/actives.pkl.gz\")\n",
    "    if target == \"clogP\":\n",
    "        reference['cfp'] = applyParallel( reference[\"SMILES\"], generate_fingerprint_series)\n",
    "\n",
    "    reference_training = reference.query(\"trainingset_class == 'training'\").copy()\n",
    "\n",
    "    reference_test =  reference.query(\"trainingset_class == 'test' or trainingset_class == 'validation'\").copy()\n",
    "\n",
    "    \n",
    "    for filtername in filternames:\n",
    "        if not os.path.exists(f\"data/memories/{target}_{filtername}/memory_with_nn.pkl.gz\") or OVERWRITE_FILES:\n",
    "            memory = pd.read_csv(f\"data/memories/{target}_{filtername}/memory_preprocessed.csv.gz\")\n",
    "            memory.rename(columns={\"step\": \"STEP\", \"total_score\": \"SCORE\"}, inplace=True)\n",
    "            memory['cfp'] = applyParallel( memory[\"SMILES\"], generate_fingerprint_series)\n",
    "\n",
    "            nns_arr = find_nns_from_pandas(memory['cfp'], reference['cfp'])\n",
    "\n",
    "            memory[\"NN_dist\"] = nns_arr[:,0]\n",
    "            memory[\"NN_idx\"] = nns_arr[:,1]\n",
    "            memory[\"NN_avg\"] = nns_arr[:,2]\n",
    "            memory[\"NN_Original_Entry_ID\"] = reference.iloc[nns_arr[:,1]][\"Original_Entry_ID\"].values\n",
    "            memory[\"NN_RDKIT_SMILES\"] = reference.iloc[nns_arr[:,1]][\"RDKIT_SMILES\"].values\n",
    "            memory[\"NN_ID\"] = reference.iloc[nns_arr[:,1]][\"ID\"].values\n",
    "\n",
    "            nns_arr = find_nns_from_pandas(memory['cfp'], reference_training['cfp'])\n",
    "\n",
    "            memory[\"NN_dist_training\"] = nns_arr[:,0]\n",
    "            memory[\"NN_idx_training\"] = nns_arr[:,1]\n",
    "            memory[\"NN_avg_training\"] = nns_arr[:,2]\n",
    "            memory[\"NN_Original_Entry_ID_training\"] = reference_training.iloc[nns_arr[:,1]][\"Original_Entry_ID\"].values\n",
    "            memory[\"NN_RDKIT_SMILES_training\"] = reference_training.iloc[nns_arr[:,1]][\"RDKIT_SMILES\"].values\n",
    "            memory[\"NN_ID_training\"] = reference_training.iloc[nns_arr[:,1]][\"ID\"].values\n",
    "\n",
    "            nns_arr = find_nns_from_pandas(memory['cfp'], reference_test['cfp'])\n",
    "\n",
    "            memory[\"NN_dist_test\"] = nns_arr[:,0]\n",
    "            memory[\"NN_idx_test\"] = nns_arr[:,1]\n",
    "            memory[\"NN_avg_test\"] = nns_arr[:,2]\n",
    "            memory[\"NN_Original_Entry_ID_test\"] = reference_test.iloc[nns_arr[:,1]][\"Original_Entry_ID\"].values\n",
    "            memory[\"NN_RDKIT_SMILES_test\"] = reference_test.iloc[nns_arr[:,1]][\"RDKIT_SMILES\"].values\n",
    "            memory[\"NN_ID__test\"] = reference_test.iloc[nns_arr[:,1]][\"ID\"].values\n",
    "\n",
    "            memory.to_pickle(f\"data/memories/{target}_{filtername}/memory_with_nn.pkl.gz\")\n",
    "        else:\n",
    "            print(f\"Skipping data/memories/{target}_{filtername}/memory_with_nn.pkl.gz as it seems to already be processed\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumsum_per_target(memory, STEPS, whattoplot=\"SMILES\"):\n",
    "    nb_stuff = []\n",
    "    stuff = set()\n",
    "\n",
    "    for i in range(1,STEPS+1):\n",
    "        subset = memory[memory[\"STEP\"] == i-1]\n",
    "        for s in subset[whattoplot]:\n",
    "            stuff.add(s)\n",
    "        nb_stuff.append(len(stuff))\n",
    "    \n",
    "    return nb_stuff\n",
    "\n",
    "cumsum = dict()\n",
    "memories = dict()\n",
    "for target, params in target_params.items():\n",
    "    cumsum[target] = dict()\n",
    "    memories[target] = dict()\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        memory = pd.read_pickle(f\"data/memories/{target}_{filtername}/memory_with_nn.pkl.gz\")\n",
    "        memory.rename(columns={\"step\": \"STEP\", \"total_score\": \"SCORE\"}, inplace=True)\n",
    "        memories[target][filtername] = memory\n",
    "        subset = memory.query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        cumsum[target][filtername] = dict()\n",
    "        for stuff in [\"SMILES\", \"Murcko Scaffold\", \"Topological Scaffold\"]: \n",
    "            subset = subset.query(\"NN_dist >= 0.4\")\n",
    "            cumsum[target][filtername][stuff] = get_cumsum_per_target(subset, maxstep, stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of unique actives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        nb = len(set(subset[\"SMILES\"]))\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of unique Murcko Scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        nb = len(set(subset[\"Murcko Scaffold\"]))\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of unique Topological Scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        nb = len(set(subset[\"Topological Scaffold\"]))\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of exact SMILES matches from ExCAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    \n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    if target == 'clogP':\n",
    "        continue\n",
    "    df = pd.read_pickle(f\"data/{target}/actives.pkl.gz\")\n",
    "    actives = set(df[\"RDKIT_SMILES\"])\n",
    "\n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        \n",
    "        generated = set(subset[\"SMILES\"])\n",
    "        nb_overlap = len(actives.intersection(generated))\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb_overlap}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of ECFP6 analogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        subset = subset.drop_duplicates(\"SMILES\")\n",
    "        nb = len(subset.query(\"NN_dist >= 0.4\"))\n",
    "        percent = (nb / len(subset))*100\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\\t{percent:.4}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        subset = subset.drop_duplicates(\"SMILES\")\n",
    "        nb = len(subset.query(\"NN_dist_training >= 0.4\"))\n",
    "        percent = (nb / len(subset))*100\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\\t{percent:.4}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, params in target_params.items():\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity']    \n",
    "    for filtername in filternames:\n",
    "        subset = memories[target][filtername].query(\"STEP < @maxstep and SCORE >= @minactivity\")\n",
    "        subset = subset.drop_duplicates(\"SMILES\")\n",
    "        nb = len(subset.query(\"NN_dist_test >= 0.4\"))\n",
    "        percent = (nb / len(subset))*100\n",
    "        print(f\"{target}\\t{filtername:30}\\t{nb}\\t{percent:.4}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "os.makedirs(f\"figures/\", exist_ok=True)\n",
    "# SMALL_SIZE = 8\n",
    "# MEDIUM_SIZE = 10\n",
    "# BIGGER_SIZE = 12\n",
    "\n",
    "# plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "# plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "filternames_in_plots = [\"No memory\", \"CompoundSimilarity memory\", \"IdenticalMurckoScaffold memory\", \"IdenticalCarbonSkeleton memory\", \"ScaffoldSimilarity memory\"]\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "#plt.style.use('bmh')\n",
    "fig = plt.figure(figsize=(5,6))\n",
    "for target in target_params.keys():\n",
    "    for stuff in [\"SMILES\", \"Murcko Scaffold\", \"Topological Scaffold\"]:\n",
    "        if stuff == \"SMILES\":\n",
    "            print_ylabel = \"No. generated ECFP6 analogs\"\n",
    "        elif stuff == \"Murcko Scaffold\":\n",
    "            print_ylabel = \"No. generated Bemis Murcko scaffolds\"\n",
    "        else:\n",
    "            print_ylabel = \"No. generated carbon skeletons\"\n",
    "        for filtername in filternames:\n",
    "            plt.plot(cumsum[target][filtername][stuff])\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(print_ylabel)\n",
    "        #plt.legend(filternames_in_plots, loc='upper right', ncol=3)\n",
    "        #plt.savefig(\"{} {} without_Title.png\".format(target, stuff), dpi=300)\n",
    "        plt.title(target)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/{} {}.svg\".format(target, stuff))\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_cuts(smiles_of_context):\n",
    "    if \"[*:3]\" in smiles_of_context:\n",
    "        return 3\n",
    "    elif \"[*:2]\" in smiles_of_context:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def load_mmp(target, filtername,maxstep, minactivity):\n",
    "    file = glob(f\"MMP/{target}_{filtername}/MMP_filtered.csv\")[0]\n",
    "    df = pd.read_csv(file)\n",
    "    mmp_pairs = df.query(\"SET_OF_LEFT_MMP == @filtername and STEP < @maxstep and SCORE >= @minactivity\").copy()\n",
    "    mmp_pairs[\"cuts\"] = mmp_pairs[\"SMILES_OF_CONTEXT\"].map(get_num_cuts)\n",
    "    mmp_pairs = mmp_pairs.query(\"cuts == 1\").sort_values(by=['cuts','STEP'])\n",
    "    mmp_pairs = mmp_pairs.drop_duplicates([\"ID_OF_LEFT_MMP\",\"ID_OF_RIGHT_MMP\"], keep='first')\n",
    "    \n",
    "    if target == 'clogP':\n",
    "        target_to_load = \"DRD2\"\n",
    "    else:\n",
    "        target_to_load = target\n",
    "    target_df = pd.read_pickle(f\"data/{target_to_load}/actives.pkl.gz\")\n",
    "    \n",
    "    mmp_pairs = mmp_pairs.merge(target_df, left_on = \"SMILES_OF_RIGHT_MMP\", right_on= \"RDKIT_SMILES\")\n",
    "    return mmp_pairs\n",
    "        \n",
    "mmps = {} \n",
    "for target, params in target_params.items():\n",
    "    if target not in mmps:\n",
    "        mmps[target] = {}\n",
    "    maxstep = params['maxstep']\n",
    "    minactivity = params['minactivity'] \n",
    "    \n",
    "    for filtername in filternames:\n",
    "        mmps[target][filtername] = {}\n",
    "        mmp = load_mmp(target, filtername, maxstep, minactivity)          \n",
    "        mmps[target][filtername] = mmp[['SMILES_OF_LEFT_MMP', 'SMILES_OF_RIGHT_MMP', 'ID_OF_LEFT_MMP', 'ID_OF_RIGHT_MMP', 'SMIRKS_OF_TRANSFORMATION', 'SMILES_OF_CONTEXT', 'STEP', 'Original_Entry_ID', 'DB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools\n",
    "for target, params in target_params.items():\n",
    "    os.makedirs(f\"htmls/{target}\", exist_ok=True)\n",
    "    for filtername in filternames:\n",
    "        save_df = mmps[target][filtername][['SMILES_OF_LEFT_MMP', 'SMILES_OF_RIGHT_MMP', 'ID_OF_LEFT_MMP', 'Original_Entry_ID']].copy()\n",
    "        PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_LEFT_MMP', molCol='MOL_OF_LEFT_MMP')\n",
    "        PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_RIGHT_MMP', molCol='MOL_OF_RIGHT_MMP')\n",
    "        save_df = save_df.sort_values(by=['SMILES_OF_RIGHT_MMP'])\n",
    "        with open(f\"htmls/{target}/{target}_{filtername}_mmps.html\", 'w') as fd:\n",
    "            html = save_df.to_html()\n",
    "            fd.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools\n",
    "os.makedirs(f\"figures\", exist_ok=True)\n",
    "save_df = []\n",
    "for filtername in filternames:\n",
    "    indices = {\"NoFilter\": [1,2],\n",
    "               \"ScaffoldSimilarity\": [0,2],\n",
    "               \"CompoundSimilarity\": [1,2],\n",
    "               \"IdenticalMurckoScaffold\": [1,5],\n",
    "               \"IdenticalTopologicalScaffold\": [3,4]\n",
    "              }\n",
    "    a = mmps[\"HTR1A\"][filtername][['SMILES_OF_LEFT_MMP', 'SMILES_OF_RIGHT_MMP', 'ID_OF_LEFT_MMP', 'Original_Entry_ID']].copy()\n",
    "    a = a.query(\"Original_Entry_ID == 'CHEMBL277120'\").iloc[indices[filtername]].copy()\n",
    "    save_df.append(a)\n",
    "save_df = pd.concat(save_df)\n",
    "PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_LEFT_MMP', molCol='MOL_OF_LEFT_MMP')\n",
    "PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_RIGHT_MMP', molCol='MOL_OF_RIGHT_MMP')\n",
    "with open(f\"figures/figure4_example_mmps.html\", 'w') as fd:\n",
    "    html = save_df.to_html()\n",
    "    fd.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools\n",
    "os.makedirs(f\"figures\", exist_ok=True)\n",
    "save_df = []\n",
    "for filtername in filternames:\n",
    "    indices = {\"NoFilter\": [(\"Original_Entry_ID == '409926' or Original_Entry_ID == '2845629'\", [])],\n",
    "               \"ScaffoldSimilarity\": [(\"Original_Entry_ID == '409926'\", [2,3])],\n",
    "               \"CompoundSimilarity\": [(\"Original_Entry_ID == '409926'\", [24,26])],\n",
    "               \"IdenticalMurckoScaffold\": [(\"Original_Entry_ID == '409926'\", [0]), (\"Original_Entry_ID == '2845629'\", [3])],\n",
    "               \"IdenticalTopologicalScaffold\": [(\"Original_Entry_ID == '409926'\", [0]), (\"Original_Entry_ID == '2845629'\", [2])]\n",
    "              }\n",
    "    a = mmps[\"DRD2\"][filtername][['SMILES_OF_LEFT_MMP', 'SMILES_OF_RIGHT_MMP', 'ID_OF_LEFT_MMP', 'Original_Entry_ID']].copy()\n",
    "    for query, index in indices[filtername]:\n",
    "        x = a.query(query).iloc[index].copy()\n",
    "        save_df.append(x)\n",
    "save_df = pd.concat(save_df)\n",
    "PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_LEFT_MMP', molCol='MOL_OF_LEFT_MMP')\n",
    "PandasTools.AddMoleculeColumnToFrame(save_df, smilesCol='SMILES_OF_RIGHT_MMP', molCol='MOL_OF_RIGHT_MMP')\n",
    "#save_df = save_df.drop_duplicates(\"SMILES_OF_LEFT_MMP\",keep='first')\n",
    "with open(f\"figures/figure5_example_mmps.html\", 'w') as fd:\n",
    "    html = save_df.to_html()\n",
    "    fd.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
